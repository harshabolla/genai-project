{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7545731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "import os\n",
    "\n",
    "# Set a custom USER_AGENT to avoid warning\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (compatible; MyLangchainBot/1.0; +https://example.com/bot)\"\n",
    "## load,chunk and index the content of the html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "\n",
    "                     )))\n",
    "\n",
    "text_documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beddbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bba2944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Self-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(text_documents)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe5932dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector Embedding And Vector Store\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16b88588",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad481c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings:\n"
     ]
    }
   ],
   "source": [
    "query = \"what isShort-term memory: ?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3a5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pdf reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('temp_Privacy_Policy.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da176daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'iText 2.1.7 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-04-21T19:59:08+05:30', 'moddate': '2025-04-21T19:59:08+05:30', 'source': 'temp_Privacy_Policy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='FROM Harsha,\\nTO Cognizant Technology Solutions India Private Limited\\nI Read and Understood the below Candidate Privacy policy.\\nBefore submitting your details, please read our  Notice which provides important Candidate Privacy\\ninformation about the collection and use of your personal information for recruitment purposes, \\nincluding information on your individual rights.\\nIf your job application is unsuccessful, please let us know if you would like us to retain your details \\nso that we can keep in touch with you about other future job opportunities at Cognizant and send you \\nother useful recruitment related information. If you chose to sign up to receive this information from \\nCognizant, we will use your personal information to match you with future roles that we believe may \\nbe suitable and to send you relevant communications and campaigns via email and/or SMS. For \\nfurther information about how we will collect and use your personal information for this purpose, \\nplease read our , which supplements the Candidate Privacy Notice.Talent Search Privacy Notice\\nSupplemental Privacy Notice (Applicable Only For India).\\nCognizant Technology Solutions Corporation and its affiliated companies (“Cognizant” “we” or \\n“us”) are firmly committed to protecting your privacy. This notice is supplemental to the Candidate \\nPrivacy Notice (“CPN”) and applies only to candidates within India.\\n(Note: Please contact your recruitment manager for assistance if you are unable to access the link to \\nthe CPN)\\nWhen you apply for a role at Cognizant, we will use the personal information you provide to assess \\nyour suitability and fitness for the role using the assistance of automated processing tools. For further \\ninformation, please read our , which supplements the Talent Search Privacy Notice (“TSPN”)\\nCandidate Privacy Notice.\\nIf, at any time, you have questions or concern about us using automated processing tools to assess \\nyour application, please email us at . In addition, you may submit mailto:SAR@cognizant.com\\nconcerns or complaints to the Data Protection Officer at .DataProtectionOfficer@cognizant.com\\nDuring the recruitment process, Cognizant will collect your Permanent Account Number (“PAN”) \\nfor the purposes of processing your job application and to prevent the duplication of job applications. \\nThis is in line with Cognizant’s legitimate interest to optimize and improve its recruitment process. \\nYour PAN will only be used for the aforementioned purposes and will be protected in accordance \\nwith Cognizant’s security policies.\\nYou may choose to not disclose your PAN to us. However, please be advised that this information is \\na mandatory prerequisite to the processing of your job application. You cannot proceed further and \\nsubmit your job application if you choose not to disclose your PAN to us and submit \\nacknowledgement as mentioned below.\\nCognizant will conduct your Digital Background Validation (‘Digital Validation’) if your job \\napplication is successfully processed, and you are shortlisted. The Digital Validation will be'),\n",
       " Document(metadata={'producer': 'iText 2.1.7 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2025-04-21T19:59:08+05:30', 'moddate': '2025-04-21T19:59:08+05:30', 'source': 'temp_Privacy_Policy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='performed by Cognizant empanelled vendor(s) [‘Cognizant Vendor(s)] on behalf of Cognizant using \\nyour PAN and Universal Account Number (‘UAN’), at the pre-offer stage. The purpose of Digital \\nValidation is to avoid proxy candidates and address moonlighting issue before rolling out the offer to \\nshortlisted candidates. The Digital Validation includes identity check, salary history check and \\nemployment history check of the shortlisted candidates by extracting and analysing the relevant \\ndetails from Income Tax portal  and EPFO portal (www.(www.eportal.incometax.gov.in)\\nunifiedportal-mem.epfindia.gov.in). If shortlisted, Cognizant will share your basic identity details \\nsuch as name, candidate ID, personal phone number, personal email address, employment history \\nwith Cognizant Vendor(s) for them to initiate the Digital Validation. As part of the Digital Validation \\nprocess, Cognizant and Cognizant Vendor(s) will be purview to your personal information such as \\nemployment history as updated in the EPFO portal, salary history as per 26AS statement as updated \\nin the Income Tax Portal, UAN and PAN. If you are shortlisted, you will be provided more \\ninformation before the Digital Validation process is initiated.\\nAcknowledgment\\nIf you are located in a country outside India, by clicking ‘I Confirm’, you confirm that you \\nhave read the  and  Notice.Talent Search Privacy Notice Candidate Privacy\\nIf you are located in India, by clicking ‘I Confirm’, you hereby provide your consent to the \\nprocessing of your personal information by Cognizant in accordance with the terms of the \\nCPN, the TSPN and the Supplemental Privacy Notice. By clicking ‘I Confirm’, you further \\nacknowledge that:\\nYou have fully read and understood the CPN, TSPN and the Supplemental Privacy Notice;\\nYou voluntarily consent to the terms of the CPN, TSPN and the Supplemental Privacy Notice;\\nYou have the right to refuse to disclose your personal information; and\\nYou have the right to withdraw your consent to the processing of your personal information at any \\ntime.\\nDate : 2025-04-21 19:59:08')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c754130",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8747dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7698836",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Introduction\n",
    "## Create Stuff Docment Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "960e913c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001AD382CCF90>, search_kwargs={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n",
    "\"\"\"\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41521a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad256e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsha Goud\\AppData\\Local\\Temp\\ipykernel_1476\\2640383322.py:8: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": \"What is the purpose of the Privacy Policy?\"})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                    llm=llm,\n",
    "                    retriever=retriever,\n",
    "                    return_source_documents=True\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8c3f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"question\": \"WHAT ARE DIFFERENT TYPES OF PROmpt \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fd40903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Based on the provided text, here are different types of prompts:\\n\\n*   Chain of Thought (CoT): Instructs the model to \"think step by step\" to decompose tasks into smaller steps. (https://lilianweng.github.io/posts/2023-06-23-agent/)\\n*   Tree of Thoughts: Extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. (https://lilianweng.github.io/posts/2023-06-23-agent/)\\n*   ReAct: Integrates reasoning and acting within LLMs, prompting the model to think, act, and observe. (https://lilianweng.github.io/posts/2023-06-23-agent/)\\n\\n',\n",
       " 'https://lilianweng.github.io/posts/2023-06-23-agent/')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"], result[\"sources\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9491b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa2260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
